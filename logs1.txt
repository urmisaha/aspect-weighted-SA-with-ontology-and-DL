Processing text dataset
len(aspect_term_list) 1295
counts:
overall positive sentences:  2389
overall negative sentences:  655
food_count =  2030    pos:  1561   neg:  469
service_count =  933  pos:  516    neg:  417
price_count =  427  pos:  242      neg:  185
ambience_count =  666    pos:  433   neg:  233
misc_count =  2125    pos:  1705   neg:  420

matrix:
food:  {'pp': 0.7571428571428571, 'pn': 0.011822660098522168, 'np': 0.06945812807881774, 'nn': 0.16157635467980297}
service:  {'pp': 0.5423365487674169, 'pn': 0.010718113612004287, 'np': 0.10503751339764202, 'nn': 0.34190782422293675}
price:  {'pp': 0.5597189695550351, 'pn': 0.00702576112412178, 'np': 0.12646370023419204, 'nn': 0.30679156908665106}
ambience:  {'pp': 0.6351351351351351, 'pn': 0.015015015015015015, 'np': 0.13963963963963963, 'nn': 0.21021021021021022}
misc:  {'pp': 0.7905882352941176, 'pn': 0.011764705882352941, 'np': 0.03811764705882353, 'nn': 0.1595294117647059}

aspect_weights:
{'food': 0.9187192118226601, 'service': 0.8842443729903536, 'price': 0.8665105386416861, 'ambience': 0.8453453453453453, 'misc': 0.9501176470588235}
(2283, 2)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 40, 20)            88860     
_________________________________________________________________
lstm_1 (LSTM)                (None, 100)               48400     
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 202       
=================================================================
Total params: 137,462
Trainable params: 137,462
Non-trainable params: 0
_________________________________________________________________
None
Train on 2283 samples, validate on 2283 samples
Epoch 1/5

 128/2283 [>.............................] - ETA: 12s - loss: 0.9189 - acc: 0.2109
 256/2283 [==>...........................] - ETA: 6s - loss: 0.8434 - acc: 0.2422 
 384/2283 [====>.........................] - ETA: 4s - loss: 0.7869 - acc: 0.3411
 512/2283 [=====>........................] - ETA: 3s - loss: 0.7422 - acc: 0.4434
 768/2283 [=========>....................] - ETA: 2s - loss: 0.6824 - acc: 0.5469
 896/2283 [==========>...................] - ETA: 1s - loss: 0.6649 - acc: 0.5770
1152/2283 [==============>...............] - ETA: 1s - loss: 0.6209 - acc: 0.6311
1408/2283 [=================>............] - ETA: 0s - loss: 0.5960 - acc: 0.6655
1536/2283 [===================>..........] - ETA: 0s - loss: 0.5888 - acc: 0.6777
1664/2283 [====================>.........] - ETA: 0s - loss: 0.5816 - acc: 0.6881
1920/2283 [========================>.....] - ETA: 0s - loss: 0.5722 - acc: 0.7031
2048/2283 [=========================>....] - ETA: 0s - loss: 0.5727 - acc: 0.7056
2176/2283 [===========================>..] - ETA: 0s - loss: 0.5755 - acc: 0.7068
2283/2283 [==============================] - 2s 899us/step - loss: 0.5718 - acc: 0.7113 - val_loss: 0.5151 - val_acc: 0.7889
Epoch 2/5

 128/2283 [>.............................] - ETA: 1s - loss: 0.4967 - acc: 0.7969
 384/2283 [====>.........................] - ETA: 0s - loss: 0.5281 - acc: 0.7734
 640/2283 [=======>......................] - ETA: 0s - loss: 0.5201 - acc: 0.7875
 896/2283 [==========>...................] - ETA: 0s - loss: 0.5232 - acc: 0.7857
1152/2283 [==============>...............] - ETA: 0s - loss: 0.5248 - acc: 0.7830
1408/2283 [=================>............] - ETA: 0s - loss: 0.5127 - acc: 0.7955
1536/2283 [===================>..........] - ETA: 0s - loss: 0.5146 - acc: 0.7943
1792/2283 [======================>.......] - ETA: 0s - loss: 0.5216 - acc: 0.7874
1920/2283 [========================>.....] - ETA: 0s - loss: 0.5176 - acc: 0.7901
2048/2283 [=========================>....] - ETA: 0s - loss: 0.5205 - acc: 0.7866
2283/2283 [==============================] - 1s 522us/step - loss: 0.5178 - acc: 0.7889 - val_loss: 0.5126 - val_acc: 0.7889
Epoch 3/5

 128/2283 [>.............................] - ETA: 0s - loss: 0.5091 - acc: 0.7812
 256/2283 [==>...........................] - ETA: 0s - loss: 0.5044 - acc: 0.7930
 384/2283 [====>.........................] - ETA: 0s - loss: 0.4903 - acc: 0.8073
 512/2283 [=====>........................] - ETA: 0s - loss: 0.4693 - acc: 0.8184
 640/2283 [=======>......................] - ETA: 0s - loss: 0.4883 - acc: 0.8047
 768/2283 [=========>....................] - ETA: 0s - loss: 0.4867 - acc: 0.8060
 896/2283 [==========>...................] - ETA: 0s - loss: 0.4764 - acc: 0.8125
1024/2283 [============>.................] - ETA: 0s - loss: 0.4893 - acc: 0.8047
1152/2283 [==============>...............] - ETA: 0s - loss: 0.4926 - acc: 0.8030
1280/2283 [===============>..............] - ETA: 0s - loss: 0.4911 - acc: 0.8031
1408/2283 [=================>............] - ETA: 0s - loss: 0.4928 - acc: 0.8026
1536/2283 [===================>..........] - ETA: 0s - loss: 0.4955 - acc: 0.8008
1792/2283 [======================>.......] - ETA: 0s - loss: 0.5032 - acc: 0.7958
2048/2283 [=========================>....] - ETA: 0s - loss: 0.5100 - acc: 0.7905
2283/2283 [==============================] - 1s 548us/step - loss: 0.5121 - acc: 0.7889 - val_loss: 0.5106 - val_acc: 0.7889
Epoch 4/5

 128/2283 [>.............................] - ETA: 1s - loss: 0.5414 - acc: 0.7656
 256/2283 [==>...........................] - ETA: 0s - loss: 0.5285 - acc: 0.7734
 512/2283 [=====>........................] - ETA: 0s - loss: 0.5471 - acc: 0.7539
 640/2283 [=======>......................] - ETA: 0s - loss: 0.5505 - acc: 0.7531
 768/2283 [=========>....................] - ETA: 0s - loss: 0.5452 - acc: 0.7578
1024/2283 [============>.................] - ETA: 0s - loss: 0.5443 - acc: 0.7588
1280/2283 [===============>..............] - ETA: 0s - loss: 0.5352 - acc: 0.7672
1408/2283 [=================>............] - ETA: 0s - loss: 0.5284 - acc: 0.7741
1536/2283 [===================>..........] - ETA: 0s - loss: 0.5243 - acc: 0.7773
1664/2283 [====================>.........] - ETA: 0s - loss: 0.5167 - acc: 0.7843
1792/2283 [======================>.......] - ETA: 0s - loss: 0.5183 - acc: 0.7829
1920/2283 [========================>.....] - ETA: 0s - loss: 0.5127 - acc: 0.7870
2048/2283 [=========================>....] - ETA: 0s - loss: 0.5123 - acc: 0.7876
2283/2283 [==============================] - 1s 571us/step - loss: 0.5109 - acc: 0.7889 - val_loss: 0.5125 - val_acc: 0.7889
Epoch 5/5

 128/2283 [>.............................] - ETA: 0s - loss: 0.5376 - acc: 0.7812
 384/2283 [====>.........................] - ETA: 0s - loss: 0.5233 - acc: 0.7865
 640/2283 [=======>......................] - ETA: 0s - loss: 0.4961 - acc: 0.8016
 896/2283 [==========>...................] - ETA: 0s - loss: 0.5081 - acc: 0.7924
1024/2283 [============>.................] - ETA: 0s - loss: 0.5086 - acc: 0.7920
1152/2283 [==============>...............] - ETA: 0s - loss: 0.4998 - acc: 0.7977
1408/2283 [=================>............] - ETA: 0s - loss: 0.5037 - acc: 0.7933
1536/2283 [===================>..........] - ETA: 0s - loss: 0.5060 - acc: 0.7917
1792/2283 [======================>.......] - ETA: 0s - loss: 0.5125 - acc: 0.7852
1920/2283 [========================>.....] - ETA: 0s - loss: 0.5142 - acc: 0.7844
2176/2283 [===========================>..] - ETA: 0s - loss: 0.5123 - acc: 0.7849
2283/2283 [==============================] - 1s 535us/step - loss: 0.5078 - acc: 0.7889 - val_loss: 0.5059 - val_acc: 0.7889
predictions =  [[0.23762855 0.7623715 ]
 [0.28927836 0.7107217 ]
 [0.27223283 0.72776717]
 ...
 [0.23966421 0.76033574]
 [0.2069551  0.79304487]
 [0.22368282 0.7763172 ]]
predictions1 =  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Scores calculated from sklearn::
accuracy_score:  0.22733245729303547
precision_score:  0.0
recall_score:  0.0

Classification Report:
             precision    recall  f1-score   support

          0       0.23      1.00      0.37       173
          1       0.00      0.00      0.00       588

avg / total       0.05      0.23      0.08       761

