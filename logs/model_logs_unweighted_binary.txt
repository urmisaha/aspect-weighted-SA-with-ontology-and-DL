Processing text dataset
len(aspect_term_list) 466

aspect_weights:
{'food': 0.9414893617021276, 'service': 0.9362549800796812, 'price': 0.8719999999999999, 'ambience': 0.7804878048780488, 'misc': 0.9691252144082332}
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0  259    1 1046  311   34]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    7  260  380 1047   26   19   20   12    5 1048    0]]
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0  282    1  143    9   39  496    4   91    0]
 [   0    0    0    0    0    0    0    0    0    0    0  933   35  380
  1028    1  114    0    0    9    1   14   72    9  821  569   24  553
     6   27  124  101   11   46    1  104    0    8    0    0]]
predicted_positives =  Tensor("metrics/precision/Sum_1:0", shape=(), dtype=float32)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 40, 20)            42380     
_________________________________________________________________
lstm_1 (LSTM)                (None, 100)               48400     
_________________________________________________________________
dense_1 (Dense)              (None, 100)               10100     
_________________________________________________________________
batch_normalization_1 (Batch (None, 100)               400       
_________________________________________________________________
dropout_1 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 101       
=================================================================
Total params: 101,381
Trainable params: 101,181
Non-trainable params: 200
_________________________________________________________________
None
Train on 624 samples, validate on 624 samples
Epoch 1/5

128/624 [=====>........................] - ETA: 4s - loss: 0.7094 - acc: 0.5312 - precision: 0.8060
256/624 [===========>..................] - ETA: 1s - loss: 0.7052 - acc: 0.5156 - precision: 0.7740
384/624 [=================>............] - ETA: 0s - loss: 0.7005 - acc: 0.5391 - precision: 0.7930
512/624 [=======================>......] - ETA: 0s - loss: 0.6975 - acc: 0.5430 - precision: 0.7896
624/624 [==============================] - 2s 3ms/step - loss: 0.6979 - acc: 0.5353 - precision: 0.7866 - val_loss: 0.5778 - val_acc: 0.7724 - val_precision: 0.7724
Epoch 2/5

128/624 [=====>........................] - ETA: 0s - loss: 0.6717 - acc: 0.6250 - precision: 0.8023
256/624 [===========>..................] - ETA: 0s - loss: 0.6767 - acc: 0.5977 - precision: 0.7839
384/624 [=================>............] - ETA: 0s - loss: 0.6754 - acc: 0.6172 - precision: 0.7930
512/624 [=======================>......] - ETA: 0s - loss: 0.6739 - acc: 0.6230 - precision: 0.7889
624/624 [==============================] - 0s 640us/step - loss: 0.6686 - acc: 0.6426 - precision: 0.7916 - val_loss: 0.7363 - val_acc: 0.3638 - val_precision: 0.8929
Epoch 3/5

128/624 [=====>........................] - ETA: 0s - loss: 0.6484 - acc: 0.7344 - precision: 0.8208
256/624 [===========>..................] - ETA: 0s - loss: 0.6509 - acc: 0.7266 - precision: 0.8114
384/624 [=================>............] - ETA: 0s - loss: 0.6481 - acc: 0.7005 - precision: 0.8098
512/624 [=======================>......] - ETA: 0s - loss: 0.6487 - acc: 0.6875 - precision: 0.8084
624/624 [==============================] - 0s 616us/step - loss: 0.6446 - acc: 0.6907 - precision: 0.8040 - val_loss: 0.6010 - val_acc: 0.8349 - val_precision: 0.8343
Epoch 4/5

128/624 [=====>........................] - ETA: 0s - loss: 0.6090 - acc: 0.8125 - precision: 0.8776
256/624 [===========>..................] - ETA: 0s - loss: 0.6022 - acc: 0.8047 - precision: 0.8524
384/624 [=================>............] - ETA: 0s - loss: 0.6039 - acc: 0.7812 - precision: 0.8410
512/624 [=======================>......] - ETA: 0s - loss: 0.6001 - acc: 0.7969 - precision: 0.8489
624/624 [==============================] - 0s 564us/step - loss: 0.5967 - acc: 0.7997 - precision: 0.8536 - val_loss: 0.5134 - val_acc: 0.8093 - val_precision: 0.8038
Epoch 5/5

128/624 [=====>........................] - ETA: 0s - loss: 0.5672 - acc: 0.8125 - precision: 0.8774
256/624 [===========>..................] - ETA: 0s - loss: 0.5660 - acc: 0.8125 - precision: 0.8666
384/624 [=================>............] - ETA: 0s - loss: 0.5572 - acc: 0.8385 - precision: 0.8841
512/624 [=======================>......] - ETA: 0s - loss: 0.5470 - acc: 0.8477 - precision: 0.8922
624/624 [==============================] - 0s 604us/step - loss: 0.5377 - acc: 0.8574 - precision: 0.9030 - val_loss: 0.4972 - val_acc: 0.8686 - val_precision: 0.9524
Y_test =  [0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1
 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1
 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1
 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1
 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1
 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]
predictions1 =  [0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]
Scores calculated from sklearn::
accuracy_score:  0.7403846153846154
precision_score:  0.8192771084337349
recall_score:  0.85
