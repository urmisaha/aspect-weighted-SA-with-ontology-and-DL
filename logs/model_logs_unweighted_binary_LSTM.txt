Processing text dataset
len(aspect_term_list) 748
3121
2777
1634
2534
583
2846
1571
1458
3161
2391
986
296
1793
2200
3195
3119
3359
70
1829
2935
1805
2559
716
2464
385
1350
3534
1407
565
387
1314
675
2227
3367
1210
2191
270
638
500
1108
3294
388
3070
3373
425
3192
1098
2173
391
263
2384
2962
3404
2599
673
648
863
1623
2177
430
1101
1823
324
2783
2983
66
627
3219
2934
2787
825
197
604
2911
3074
1479
1074
912
728
2925
495
3436
3273
1215
183
2511
282
1562
1712
3249
2052
3207
672
2972
1903
2549
1755
2890
2277
2308
51
3680
3155
1688
598
2926
3083
1388
2977
1574
498
2604
473
380
2960
322
1383
892
2296
868
2936
3423
154
591
1197
2061
2274
2804
194
148
2747
2845
187
856
2033
1402
439
3360
2949
570
2053
3316
433
738
1549
1798
2941
362
2921
1909
1878
858
3376
173
1836
1703
3564
1404
645
3288
1819
1144
2639
1474
1553
2059
926
2385
2498
2469
567
3517
2069
1599
2860
1092
2548
759
3175
2209
2717
1352
1543
3537
3213
1743
1321
2907
1890
2675
2588
2234
3130
3285
709
1198
2380
2250
2302
1110
973
2132
231
1184
250
227
3653
2988
3092
1166
3686
1922
2978
2139
2454
1881
633
268
3613
1531
603
678
2690
1830
1408
2521
1918
2930
2166
887
2448
3244
1478
1413
3129
2271
1554
2306
1975
3699
2875
2303
2513
458
809
141
3655
2098
1624
1928
2377
3209
338
1630
1907
1557
3408
1535
3202
449
1449
927
585
1927
1367
701
345
293
1720
1494
1085
2636
1179
1976
1315
3194
688
2393
1278
2201
624
3252
620
3541
865
375
2126
2078
846
2158
1578
1282
1319
1175
1115
3396
34
798
395
1838
176
3102
984
2138
1472
1969
1674
1330
763
332
754
2248
691
3363
999
2503
1310
940
247
2823
3628
349
987
1611
2649
2396
139
841
3477
442
90
298
1104
2563
1280
2790
323
24
1891
2417
2980
3451
1551
3146
1785
1492
406
3650
2985
2084
2536
1820
1826
96
3584
3193
93
2212
3426
1138
1461
128
1008
241
1581
1815
1746
2124
2919
2360
3082
1950
2803
1133
3204
1728
2801
2140
2447
224
2434
459
3042
152
3062
2426
225
2424
494
1033
496
2486
2307
1646
3255
2229
2615
318
895
198
2864
1269
95
2062
3557
38
1957
1542
55
168
2328
3644
1214
1825
504
1853
1538
2482
791
998
2872
3281
2213
2607
1272
3369
3578
572
2500
45
2445
3217
1188
1973
1334
2392
1791
1044
925
1828
2363
327
2831
2479
1374
985
3061
1843
2102
2149
964
748
766
2674
2501
2222
2786
773
368
994
366
3075
1270
3664
3128
2097
1705
3145
790
9
3085
1448
1933
1661
1817
2512
1749
3441
3260
2910
76
1971
1286
797
2315
344
1990
2900
3122
719
3669
3295
1130
2896
2299
1652
2128
2835
778
3029
1672
1125
3694
2721
884
954
2452
1250
3234
2494
1533
1055
1784
2178
1689
771
3370
823
1988
3535
2449
993
3598
2840
1034
2709
2734
174
3654
1049
2515
3320
102
2397
511
1203
1077
133
3230
2905
491
2001
1362
796
2733
178
2627
2946
554
1616
3189
3693
1103
1732
990
1994
541
229
3542
1617
3322
1022
751
1711
2179
1131
2723
2012
1684
3024
1622
1059
2347
3067
530
447
1592
3191
3433
2134
2947
2770
1989
445
653
3278
2789
440
1331
2497
2705
3105
1495
3039
75
2446
3429
3637
861
2000
1316
2450
1577
3538
2901
908
1790
150
2523
2170
1012
1626
1579
1567
3515
1698
1873
2592
3081
2806
3142
3575
1884
3381
2507
745
3100
2802
1769
201
1180
2335
1593
3631
901
3049
1242
932
2769
1105
2609
776
272
1152
2894
2591
833
461
2830
1356
539
836
2533
1435
3208
3126
1455
2225
1056
3582
2708
275
2105
2152
191
3319
3681
2171
3462
1087
1865
1380
3421
1293
3475
818
2893
3170
2932
2575
451
3700
119
2817
2517
2883
556
563
924
3113
2164
3461
3041
2459
1013
193
1569
2217
78
2280
3424
3641
1707
2766
667
1600
134
3336
2421
159
1490
3546
1548
2107
1709
1550
3305
874
3116
2707
2630
3353
2081
533
972
3420
2329
1827
692
217
878
2891
2673
2276
3199
3414
209
3077
1228
3297
1759
2966
2077
3554
410
202
3492
537
2439
3562
3280
2189
3573
277
2460
952
2811
2024
3020
3079
2716
746
2912
3566
1702
199
882
2324
2524
628
3563
727
1925
2740
3422
3144
2202
618
1919
1487
2120
1868
770
2205
948
1940
2451
56
1632
1961
710
911
3607
1955
2903
1887
3495
1273
590
762
3374
179
109
339
489
2487
499
2589
3701
381
3050
1185
1555
1112
3368
1032
1124
1663
1690
788
1780
742
1471
1789
2751
1189
3352
3315
3393
2157
3018
1401
257
813
2862
3292
3343
3045
3135
3391
3651
2018
3551
2355
3178
586
2211
2317
175
1609
2706
1860
961
1057
3188
65
2735
2882
1948
2041
1046
1656
3440
870
2986
288
284
3418
3358
83
2940
2141
1854
396
3471
3577
2283
614
3012
2621
605
2017
2974
2456
25
520
3383
2186
3478
1283
3411
3221
1923
756
1194
3372
123
2522
2386
3332
717
680
1345
3013
1159
3547
792
2015
92
548
3692
1209
443
400
1486
1377
1439
2741
370
599
803
1750
737
847
81
2144
1007
3687
1161
3123
2301
3405
3576
3702
1911
703
619
907
1000
3241
1866
2570
280
1021
1729
610
3211
2984
1429
1855
3409
579
3670
685
428
3378
779
916
2194
1066
127
427
2298
269
3169
3335
2356
1199
2444
720
340
228
651
2664
1930
2090
977
2433
1949
2757
301
336
942
2887
80
2398
53
1475
1560
2025
1760
259
2829
1211
177
1312
1565
2959
3339
1659
839
457
1306
3086
2466
531
450
121
3263
2939
1678
2606
2019
1797
404
1731
2791
3261
1586
1893
309
2290
3601
736
2784
2825
1741
2118
84
369
3279
1425
2743
3324
1792
166
1679
1727
2425
3532
3455
3508
1541
1037
2288
2955
2585
1386
1378
438
232
2764
3246
2473
3051
3621
2579
3489
3600
3519
1183
2663
857
1118
2484
576
2915
3166
662
3599
502
981
1987
2889
1287
476
3626
1094
2881
285
700
1493
1014
674
953
405
3141
466
1899
729
487
1434
3410
1050
1981
1991
2035
3587
2851
2294
2726
1680
3605
3624
1671
2767
1155
3327
1700
3589
898
2429
483
359
2795
342
1239
767
3157
2123
1861
2888
2938
2544
408
3703
129
529
3504
475
2568
1677
2554
1412
2251
808
991
2284
3531
2654
1869
550
3243
1995
2316
486
1290
3469
3574
1662
2584
2578
3205
528
82
2640
1176
3237
291
3643
40
3111
1340
1111
2622
3259
1279
2495
469
3040
513
3011
3454
3248
2058
3291
3190
2693
3127
1998
1904
454
772
3676
3667
2520
668
1006
2401
3460
1621
2204
2003
1041
1089
1236
3710
663
276
1532
1277
222
1509
2220
518
2595
2260
2885
2967
3147
2187
3639
2852
221
2913
297
2964
2502
28
1654
131
3218
1874
616
3611
1580
2924
2414
2218
1436
2279
2975
1568
3496
3084
3033
3321
699
3173
1370
3340
3334
2463
2297
1353
3274
2668
2538
3060
2221
2582
2071
1645
2257
743
765
2133
1054
1040
2278
1324
1477
3059
2365
264
3525
485
731
3025
239
2765
686
1564
1069
3118
1814
744
1382
3665
1128
780
2819
566
2646
1576
3223
3214
3434
2687
2064
1191
626
1673
1706
1879
2637
3623
2420
2605
1415
1421
3660
1945
612
44
2916
859
855
514
750
2645
2267
2453
2193
1392
4
240
2030
3000
2842
2005
3239
1660
1145
2855
652
2774
1575
2908
423
1608
835
1431
377
3222
125
3342
2808
1298
2183
2841
1418
2208
287
814
3112
2413
3435
914
2695
664
1644
2036
1880
2101
2113
1822
2537
1594
3459
3572
845
3004
2441
965
1327
1615
2681
3361
789
782
3148
2203
3661
1403
3682
2727
2293
2119
205
655
2092
749
132
3176
367
3397
3382
2850
708
978
1029
3470
3034
3180
163
1017
3277
757
923
472
2223
849
1035
3196
1339
3131
3649
1140
2337
1174
3474
2818
2920
162
1216
3530
951
1005
1619
1304
1856
3318
1335
2545
1821
880
401
1238
889
21
2944
3298
1344
1381
581
3658
1840
2702
3536
1363
1065
2137
2884
2075
3238
558
3597
1099
169
2042
1603
1114
233
1832
3276
960
3555
2863
2477
1095

aspect_weights:
{'food': 0.9290573372206025, 'service': 0.9300699300699301, 'price': 0.873170731707317, 'ambience': 0.822742474916388, 'misc': 0.952952952952953}
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 40, 20)            58700     
_________________________________________________________________
lstm_1 (LSTM)                (None, 100)               48400     
_________________________________________________________________
dense_1 (Dense)              (None, 100)               10100     
_________________________________________________________________
batch_normalization_1 (Batch (None, 100)               400       
_________________________________________________________________
dropout_1 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 101       
=================================================================
Total params: 117,701
Trainable params: 117,501
Non-trainable params: 200
_________________________________________________________________
None
Train on 1096 samples, validate on 1096 samples
Epoch 1/5

 128/1096 [==>...........................] - ETA: 9s - loss: 0.7616 - acc: 0.4453 - precision: 0.7377 - recall: 0.4500 - f2_score: 0.3516
 256/1096 [======>.......................] - ETA: 4s - loss: 0.7856 - acc: 0.4297 - precision: 0.7158 - recall: 0.4059 - f2_score: 0.3086
 384/1096 [=========>....................] - ETA: 2s - loss: 0.7537 - acc: 0.4766 - precision: 0.7621 - recall: 0.4490 - f2_score: 0.3438
 512/1096 [=============>................] - ETA: 1s - loss: 0.7409 - acc: 0.5039 - precision: 0.7765 - recall: 0.4858 - f2_score: 0.3730
 640/1096 [================>.............] - ETA: 1s - loss: 0.7374 - acc: 0.5000 - precision: 0.7626 - recall: 0.4797 - f2_score: 0.3625
 768/1096 [====================>.........] - ETA: 0s - loss: 0.7410 - acc: 0.5000 - precision: 0.7645 - recall: 0.4814 - f2_score: 0.3646
1024/1096 [===========================>..] - ETA: 0s - loss: 0.7334 - acc: 0.5078 - precision: 0.7630 - recall: 0.5046 - f2_score: 0.3838
1096/1096 [==============================] - 2s 2ms/step - loss: 0.7296 - acc: 0.5173 - precision: 0.7685 - recall: 0.5188 - f2_score: 0.3987 - val_loss: 0.7265 - val_acc: 0.3212 - val_precision: 0.8860 - val_recall: 0.1325 - val_f2_score: 0.1013
Epoch 2/5

 128/1096 [==>...........................] - ETA: 0s - loss: 0.7172 - acc: 0.5156 - precision: 0.7237 - recall: 0.5729 - f2_score: 0.4297
 256/1096 [======>.......................] - ETA: 0s - loss: 0.7157 - acc: 0.5547 - precision: 0.7412 - recall: 0.6267 - f2_score: 0.4727
 512/1096 [=============>................] - ETA: 0s - loss: 0.6921 - acc: 0.6016 - precision: 0.7483 - recall: 0.6912 - f2_score: 0.5078
 640/1096 [================>.............] - ETA: 0s - loss: 0.6879 - acc: 0.6125 - precision: 0.7562 - recall: 0.7074 - f2_score: 0.5281
 896/1096 [=======================>......] - ETA: 0s - loss: 0.6742 - acc: 0.6350 - precision: 0.7778 - recall: 0.7281 - f2_score: 0.5592
1024/1096 [===========================>..] - ETA: 0s - loss: 0.6728 - acc: 0.6367 - precision: 0.7752 - recall: 0.7366 - f2_score: 0.5654
1096/1096 [==============================] - 1s 588us/step - loss: 0.6692 - acc: 0.6405 - precision: 0.7750 - recall: 0.7427 - f2_score: 0.5684 - val_loss: 0.6611 - val_acc: 0.6889 - val_precision: 0.7913 - val_recall: 0.8072 - val_f2_score: 0.6168
Epoch 3/5

 128/1096 [==>...........................] - ETA: 0s - loss: 0.6182 - acc: 0.7422 - precision: 0.8454 - recall: 0.8200 - f2_score: 0.6406
 256/1096 [======>.......................] - ETA: 0s - loss: 0.6236 - acc: 0.7344 - precision: 0.8121 - recall: 0.8590 - f2_score: 0.6641
 512/1096 [=============>................] - ETA: 0s - loss: 0.6465 - acc: 0.6973 - precision: 0.7770 - recall: 0.8384 - f2_score: 0.6289
 640/1096 [================>.............] - ETA: 0s - loss: 0.6441 - acc: 0.7000 - precision: 0.7827 - recall: 0.8396 - f2_score: 0.6391
 768/1096 [====================>.........] - ETA: 0s - loss: 0.6404 - acc: 0.6992 - precision: 0.7812 - recall: 0.8406 - f2_score: 0.6393
 896/1096 [=======================>......] - ETA: 0s - loss: 0.6346 - acc: 0.7031 - precision: 0.7847 - recall: 0.8415 - f2_score: 0.6406
1024/1096 [===========================>..] - ETA: 0s - loss: 0.6348 - acc: 0.6992 - precision: 0.7864 - recall: 0.8341 - f2_score: 0.6377
1096/1096 [==============================] - 1s 571us/step - loss: 0.6336 - acc: 0.6998 - precision: 0.7862 - recall: 0.8354 - f2_score: 0.6387 - val_loss: 0.5417 - val_acc: 0.7673 - val_precision: 0.7691 - val_recall: 0.9940 - val_f2_score: 0.7600
Epoch 4/5

 128/1096 [==>...........................] - ETA: 0s - loss: 0.6124 - acc: 0.6641 - precision: 0.7692 - recall: 0.8081 - f2_score: 0.6250
 256/1096 [======>.......................] - ETA: 0s - loss: 0.6003 - acc: 0.7070 - precision: 0.7967 - recall: 0.8403 - f2_score: 0.6602
 384/1096 [=========>....................] - ETA: 0s - loss: 0.6037 - acc: 0.7109 - precision: 0.7932 - recall: 0.8444 - f2_score: 0.6510
 512/1096 [=============>................] - ETA: 0s - loss: 0.6070 - acc: 0.7031 - precision: 0.7881 - recall: 0.8437 - f2_score: 0.6543
 640/1096 [================>.............] - ETA: 0s - loss: 0.6016 - acc: 0.7094 - precision: 0.7905 - recall: 0.8510 - f2_score: 0.6609
 768/1096 [====================>.........] - ETA: 0s - loss: 0.6047 - acc: 0.6966 - precision: 0.7778 - recall: 0.8450 - f2_score: 0.6484
1024/1096 [===========================>..] - ETA: 0s - loss: 0.6001 - acc: 0.7051 - precision: 0.7764 - recall: 0.8622 - f2_score: 0.6582
1096/1096 [==============================] - 1s 572us/step - loss: 0.5972 - acc: 0.7080 - precision: 0.7766 - recall: 0.8677 - f2_score: 0.6633 - val_loss: 0.5183 - val_acc: 0.7701 - val_precision: 0.7693 - val_recall: 0.9988 - val_f2_score: 0.7637
Epoch 5/5

 128/1096 [==>...........................] - ETA: 0s - loss: 0.5670 - acc: 0.7891 - precision: 0.8067 - recall: 0.9600 - f2_score: 0.7500
 384/1096 [=========>....................] - ETA: 0s - loss: 0.5776 - acc: 0.7578 - precision: 0.7910 - recall: 0.9284 - f2_score: 0.7109
 512/1096 [=============>................] - ETA: 0s - loss: 0.5727 - acc: 0.7617 - precision: 0.7924 - recall: 0.9337 - f2_score: 0.7168
 768/1096 [====================>.........] - ETA: 0s - loss: 0.5583 - acc: 0.7669 - precision: 0.7921 - recall: 0.9400 - f2_score: 0.7135
 896/1096 [=======================>......] - ETA: 0s - loss: 0.5520 - acc: 0.7723 - precision: 0.7956 - recall: 0.9444 - f2_score: 0.7210
1024/1096 [===========================>..] - ETA: 0s - loss: 0.5469 - acc: 0.7744 - precision: 0.7956 - recall: 0.9488 - f2_score: 0.7256
1096/1096 [==============================] - 1s 559us/step - loss: 0.5431 - acc: 0.7774 - precision: 0.7971 - recall: 0.9510 - f2_score: 0.7272 - val_loss: 0.4802 - val_acc: 0.8084 - val_precision: 0.8000 - val_recall: 0.9988 - val_f2_score: 0.7637
Y_test =  [0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1
 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1
 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1
 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1
 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1
 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1
 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1
 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1
 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0
 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1]
predictions1 =  [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Scores calculated from sklearn::
accuracy_score:  0.7534246575342466
precision_score:  0.7604456824512534
recall_score:  0.9855595667870036

Classification Report:
             precision    recall  f1-score   support

          0       0.33      0.02      0.04        88
          1       0.76      0.99      0.86       277

avg / total       0.66      0.75      0.66       365

