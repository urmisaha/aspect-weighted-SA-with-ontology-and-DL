Processing text dataset
len(aspect_term_list) 466

aspect_weights:
{'food': 0.9414893617021276, 'service': 0.9362549800796812, 'price': 0.8719999999999999, 'ambience': 0.7804878048780488, 'misc': 0.9691252144082332}
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0  259    1 1046  311   34]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    7  260  380 1047   26   19   20   12    5 1048    0]]
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0  282    1  143    9   39  496    4   91    0]
 [   0    0    0    0    0    0    0    0    0    0    0  933   35  380
  1028    1  114    0    0    9    1   14   72    9  821  569   24  553
     6   27  124  101   11   46    1  104    0    8    0    0]]
predicted_positives =  Tensor("metrics/precision/Sum_1:0", shape=(), dtype=float32)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 40, 20)            42380     
_________________________________________________________________
lstm_1 (LSTM)                (None, 100)               48400     
_________________________________________________________________
dense_1 (Dense)              (None, 100)               10100     
_________________________________________________________________
batch_normalization_1 (Batch (None, 100)               400       
_________________________________________________________________
dropout_1 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 101       
=================================================================
Total params: 101,381
Trainable params: 101,181
Non-trainable params: 200
_________________________________________________________________
None
Train on 624 samples, validate on 624 samples
Epoch 1/5

128/624 [=====>........................] - ETA: 4s - loss: 0.7205 - acc: 0.4531 - precision: 0.7385
256/624 [===========>..................] - ETA: 1s - loss: 0.7103 - acc: 0.5195 - precision: 0.7722
384/624 [=================>............] - ETA: 0s - loss: 0.7059 - acc: 0.5365 - precision: 0.7871
512/624 [=======================>......] - ETA: 0s - loss: 0.7035 - acc: 0.5312 - precision: 0.7734
624/624 [==============================] - 2s 3ms/step - loss: 0.7025 - acc: 0.5240 - precision: 0.7686 - val_loss: 0.6200 - val_acc: 0.7724 - val_precision: 0.7724
Epoch 2/5

128/624 [=====>........................] - ETA: 0s - loss: 0.6744 - acc: 0.6406 - precision: 0.8140
256/624 [===========>..................] - ETA: 0s - loss: 0.6742 - acc: 0.6602 - precision: 0.8070
384/624 [=================>............] - ETA: 0s - loss: 0.6792 - acc: 0.6224 - precision: 0.7880
512/624 [=======================>......] - ETA: 0s - loss: 0.6765 - acc: 0.6289 - precision: 0.7857
624/624 [==============================] - 0s 616us/step - loss: 0.6740 - acc: 0.6346 - precision: 0.7870 - val_loss: 0.6087 - val_acc: 0.7724 - val_precision: 0.7724
Epoch 3/5

128/624 [=====>........................] - ETA: 0s - loss: 0.6617 - acc: 0.7734 - precision: 0.8230
256/624 [===========>..................] - ETA: 0s - loss: 0.6597 - acc: 0.7109 - precision: 0.7782
384/624 [=================>............] - ETA: 0s - loss: 0.6542 - acc: 0.7161 - precision: 0.7898
512/624 [=======================>......] - ETA: 0s - loss: 0.6516 - acc: 0.7148 - precision: 0.7863
624/624 [==============================] - 0s 594us/step - loss: 0.6481 - acc: 0.7147 - precision: 0.7831 - val_loss: 0.6427 - val_acc: 0.7933 - val_precision: 0.8038
Epoch 4/5

128/624 [=====>........................] - ETA: 0s - loss: 0.6273 - acc: 0.7812 - precision: 0.8070
256/624 [===========>..................] - ETA: 0s - loss: 0.6232 - acc: 0.7812 - precision: 0.8044
384/624 [=================>............] - ETA: 0s - loss: 0.6235 - acc: 0.7682 - precision: 0.7965
512/624 [=======================>......] - ETA: 0s - loss: 0.6243 - acc: 0.7617 - precision: 0.7897
624/624 [==============================] - 0s 549us/step - loss: 0.6209 - acc: 0.7692 - precision: 0.7984 - val_loss: 0.6664 - val_acc: 0.6106 - val_precision: 0.9510
Epoch 5/5

128/624 [=====>........................] - ETA: 0s - loss: 0.5962 - acc: 0.8047 - precision: 0.8435
384/624 [=================>............] - ETA: 0s - loss: 0.5849 - acc: 0.8203 - precision: 0.8423
624/624 [==============================] - 0s 480us/step - loss: 0.5770 - acc: 0.8237 - precision: 0.8395 - val_loss: 0.5560 - val_acc: 0.8942 - val_precision: 0.9225
Y_test =  [0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1
 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1
 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1
 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1
 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1
 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]
predictions =  [[0.5159667 ]
 [0.5365173 ]
 [0.5194889 ]
 [0.53846157]
 [0.40060845]
 [0.5333468 ]
 [0.6194039 ]
 [0.5144015 ]
 [0.5702724 ]
 [0.6365631 ]
 [0.6306163 ]
 [0.55429184]
 [0.47041732]
 [0.62513083]
 [0.5676572 ]
 [0.57808006]
 [0.6539803 ]
 [0.60910565]
 [0.45937744]
 [0.59827685]
 [0.5813033 ]
 [0.6541498 ]
 [0.570559  ]
 [0.61373454]
 [0.6442025 ]
 [0.63810444]
 [0.52803814]
 [0.5958008 ]
 [0.6351542 ]
 [0.5584084 ]
 [0.55202526]
 [0.5914958 ]
 [0.5570025 ]
 [0.64111054]
 [0.52138853]
 [0.58506405]
 [0.5274397 ]
 [0.41327098]
 [0.6260072 ]
 [0.5878147 ]
 [0.6186336 ]
 [0.6241818 ]
 [0.5671817 ]
 [0.6262906 ]
 [0.61120415]
 [0.6867899 ]
 [0.5602724 ]
 [0.6091107 ]
 [0.3324749 ]
 [0.6104436 ]
 [0.6395619 ]
 [0.65034753]
 [0.5831157 ]
 [0.6637124 ]
 [0.6119126 ]
 [0.4943794 ]
 [0.5194543 ]
 [0.6940458 ]
 [0.53827363]
 [0.46735108]
 [0.5781499 ]
 [0.6151066 ]
 [0.4323312 ]
 [0.55315363]
 [0.5849182 ]
 [0.51548386]
 [0.56270146]
 [0.6488428 ]
 [0.5123549 ]
 [0.6079425 ]
 [0.62230945]
 [0.49487242]
 [0.5634314 ]
 [0.60304815]
 [0.53098977]
 [0.47799224]
 [0.5969298 ]
 [0.60481715]
 [0.5241561 ]
 [0.62645626]
 [0.54085106]
 [0.6588324 ]
 [0.6096986 ]
 [0.64263135]
 [0.5497096 ]
 [0.5142281 ]
 [0.5747932 ]
 [0.6915408 ]
 [0.5772471 ]
 [0.59841603]
 [0.61538726]
 [0.56525433]
 [0.588739  ]
 [0.4831328 ]
 [0.57976073]
 [0.56292135]
 [0.5991106 ]
 [0.5406147 ]
 [0.5555642 ]
 [0.6591414 ]
 [0.61371374]
 [0.51344216]
 [0.56764805]
 [0.46356684]
 [0.63994473]
 [0.5152243 ]
 [0.53478324]
 [0.61104894]
 [0.6268569 ]
 [0.6429723 ]
 [0.5264446 ]
 [0.6629203 ]
 [0.58310044]
 [0.5798391 ]
 [0.6255007 ]
 [0.5383915 ]
 [0.5506596 ]
 [0.5310881 ]
 [0.5250251 ]
 [0.5385001 ]
 [0.5803654 ]
 [0.5521935 ]
 [0.5724057 ]
 [0.48450455]
 [0.43154302]
 [0.5722034 ]
 [0.49120373]
 [0.5730631 ]
 [0.47053814]
 [0.5612866 ]
 [0.5353309 ]
 [0.5171013 ]
 [0.53526485]
 [0.65374976]
 [0.51626396]
 [0.45082644]
 [0.5691292 ]
 [0.595575  ]
 [0.5133433 ]
 [0.5402972 ]
 [0.54330754]
 [0.52843785]
 [0.55172133]
 [0.52339953]
 [0.502108  ]
 [0.52173173]
 [0.53092325]
 [0.6398473 ]
 [0.6254923 ]
 [0.46733537]
 [0.54940426]
 [0.5553896 ]
 [0.5749946 ]
 [0.60778636]
 [0.5601924 ]
 [0.46528932]
 [0.5985352 ]
 [0.61232895]
 [0.58551764]
 [0.49559152]
 [0.6627495 ]
 [0.57762915]
 [0.5653164 ]
 [0.4792194 ]
 [0.49066678]
 [0.60684687]
 [0.5133888 ]
 [0.5653385 ]
 [0.59083927]
 [0.6425693 ]
 [0.57322454]
 [0.59230834]
 [0.6040544 ]
 [0.66117543]
 [0.5646794 ]
 [0.6264156 ]
 [0.5671344 ]
 [0.6192414 ]
 [0.5404434 ]
 [0.5412258 ]
 [0.60938716]
 [0.5724552 ]
 [0.6198777 ]
 [0.5505545 ]
 [0.6268068 ]
 [0.5034246 ]
 [0.6393001 ]
 [0.5562155 ]
 [0.5696252 ]
 [0.6241993 ]
 [0.53388846]
 [0.49033573]
 [0.5918435 ]
 [0.56657   ]
 [0.67722726]
 [0.6637954 ]
 [0.57563454]
 [0.6495544 ]
 [0.57940465]
 [0.5889385 ]
 [0.64162445]
 [0.63608694]
 [0.5862889 ]
 [0.5619645 ]
 [0.6015068 ]
 [0.50254005]
 [0.60065496]
 [0.45935124]]
[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]
Scores calculated from sklearn::
accuracy_score:  0.75
precision_score:  0.7934782608695652
recall_score:  0.9125
