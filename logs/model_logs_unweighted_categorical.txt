Processing text dataset
len(aspect_term_list) 466

aspect_weights:
{'food': 0.9414893617021276, 'service': 0.9362549800796812, 'price': 0.8719999999999999, 'ambience': 0.7804878048780488, 'misc': 0.9691252144082332}
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0  259    1 1046  311   34]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    7  260  380 1047   26   19   20   12    5 1048    0]]
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0  282    1  143    9   39  496    4   91    0]
 [   0    0    0    0    0    0    0    0    0    0    0  933   35  380
  1028    1  114    0    0    9    1   14   72    9  821  569   24  553
     6   27  124  101   11   46    1  104    0    8    0    0]]
predicted_positives =  Tensor("metrics/precision/Sum_1:0", shape=(), dtype=float32)
possible_positives =  Tensor("metrics/recall/Sum_1:0", shape=(), dtype=float32)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 40, 20)            42380     
_________________________________________________________________
lstm_1 (LSTM)                (None, 100)               48400     
_________________________________________________________________
dense_1 (Dense)              (None, 100)               10100     
_________________________________________________________________
batch_normalization_1 (Batch (None, 100)               400       
_________________________________________________________________
dropout_1 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 202       
=================================================================
Total params: 101,482
Trainable params: 101,282
Non-trainable params: 200
_________________________________________________________________
None
Train on 624 samples, validate on 624 samples
Epoch 1/5

128/624 [=====>........................] - ETA: 7s - loss: 0.7791 - acc: 0.4609 - precision: 0.4609 - recall: 0.4609 - f2_score: 0.4609
256/624 [===========>..................] - ETA: 3s - loss: 0.7472 - acc: 0.4414 - precision: 0.4414 - recall: 0.4414 - f2_score: 0.4414
384/624 [=================>............] - ETA: 1s - loss: 0.7298 - acc: 0.4661 - precision: 0.4661 - recall: 0.4661 - f2_score: 0.4661
512/624 [=======================>......] - ETA: 0s - loss: 0.7267 - acc: 0.4844 - precision: 0.4844 - recall: 0.4844 - f2_score: 0.4844
624/624 [==============================] - 3s 4ms/step - loss: 0.7198 - acc: 0.4984 - precision: 0.4984 - recall: 0.4984 - f2_score: 0.4984 - val_loss: 0.6286 - val_acc: 0.7676 - val_precision: 0.7676 - val_recall: 0.7676 - val_f2_score: 0.7676
Epoch 2/5

128/624 [=====>........................] - ETA: 0s - loss: 0.6892 - acc: 0.5859 - precision: 0.5859 - recall: 0.5859 - f2_score: 0.5859
256/624 [===========>..................] - ETA: 0s - loss: 0.6762 - acc: 0.6172 - precision: 0.6172 - recall: 0.6172 - f2_score: 0.6172
384/624 [=================>............] - ETA: 0s - loss: 0.6725 - acc: 0.6120 - precision: 0.6120 - recall: 0.6120 - f2_score: 0.6120
512/624 [=======================>......] - ETA: 0s - loss: 0.6698 - acc: 0.6113 - precision: 0.6113 - recall: 0.6113 - f2_score: 0.6113
624/624 [==============================] - 0s 759us/step - loss: 0.6680 - acc: 0.6170 - precision: 0.6170 - recall: 0.6170 - f2_score: 0.6170 - val_loss: 0.5394 - val_acc: 0.7724 - val_precision: 0.7724 - val_recall: 0.7724 - val_f2_score: 0.7724
Epoch 3/5

128/624 [=====>........................] - ETA: 0s - loss: 0.6491 - acc: 0.7109 - precision: 0.7109 - recall: 0.7109 - f2_score: 0.7109
256/624 [===========>..................] - ETA: 0s - loss: 0.6506 - acc: 0.6953 - precision: 0.6953 - recall: 0.6953 - f2_score: 0.6953
384/624 [=================>............] - ETA: 0s - loss: 0.6430 - acc: 0.7109 - precision: 0.7109 - recall: 0.7109 - f2_score: 0.7109
512/624 [=======================>......] - ETA: 0s - loss: 0.6395 - acc: 0.7148 - precision: 0.7148 - recall: 0.7148 - f2_score: 0.7148
624/624 [==============================] - 0s 627us/step - loss: 0.6366 - acc: 0.7212 - precision: 0.7212 - recall: 0.7212 - f2_score: 0.7212 - val_loss: 0.5517 - val_acc: 0.7740 - val_precision: 0.7740 - val_recall: 0.7740 - val_f2_score: 0.7740
Epoch 4/5

128/624 [=====>........................] - ETA: 0s - loss: 0.6033 - acc: 0.7500 - precision: 0.7500 - recall: 0.7500 - f2_score: 0.7500
256/624 [===========>..................] - ETA: 0s - loss: 0.6004 - acc: 0.7422 - precision: 0.7422 - recall: 0.7422 - f2_score: 0.7422
384/624 [=================>............] - ETA: 0s - loss: 0.5971 - acc: 0.7526 - precision: 0.7526 - recall: 0.7526 - f2_score: 0.7526
512/624 [=======================>......] - ETA: 0s - loss: 0.5978 - acc: 0.7559 - precision: 0.7559 - recall: 0.7559 - f2_score: 0.7559
624/624 [==============================] - 0s 652us/step - loss: 0.5921 - acc: 0.7724 - precision: 0.7724 - recall: 0.7724 - f2_score: 0.7724 - val_loss: 0.5134 - val_acc: 0.7949 - val_precision: 0.7949 - val_recall: 0.7949 - val_f2_score: 0.7949
Epoch 5/5

128/624 [=====>........................] - ETA: 0s - loss: 0.5583 - acc: 0.8359 - precision: 0.8359 - recall: 0.8359 - f2_score: 0.8359
256/624 [===========>..................] - ETA: 0s - loss: 0.5616 - acc: 0.8242 - precision: 0.8242 - recall: 0.8242 - f2_score: 0.8242
384/624 [=================>............] - ETA: 0s - loss: 0.5532 - acc: 0.8307 - precision: 0.8307 - recall: 0.8307 - f2_score: 0.8307
512/624 [=======================>......] - ETA: 0s - loss: 0.5419 - acc: 0.8379 - precision: 0.8379 - recall: 0.8379 - f2_score: 0.8379
624/624 [==============================] - 0s 780us/step - loss: 0.5330 - acc: 0.8526 - precision: 0.8526 - recall: 0.8526 - f2_score: 0.8526 - val_loss: 0.4099 - val_acc: 0.7740 - val_precision: 0.7740 - val_recall: 0.7740 - val_f2_score: 0.7740
Y_test =  [[1. 0.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]]
[[0.26905838 0.73094165]
 [0.15274483 0.8472552 ]
 [0.2439582  0.75604177]
 [0.1953413  0.8046587 ]
 [0.3691313  0.6308687 ]
 [0.1846016  0.81539834]
 [0.1762935  0.8237065 ]
 [0.23149    0.76851   ]
 [0.15227054 0.8477295 ]
 [0.11064509 0.8893549 ]
 [0.14268228 0.85731775]
 [0.18898204 0.811018  ]
 [0.28128284 0.71871716]
 [0.12204257 0.87795746]
 [0.21495052 0.7850495 ]
 [0.19387707 0.80612296]
 [0.10318951 0.89681053]
 [0.15077142 0.8492285 ]
 [0.31674314 0.68325686]
 [0.14057095 0.85942906]
 [0.16583079 0.8341692 ]
 [0.17976728 0.82023275]
 [0.23575735 0.7642426 ]
 [0.12611397 0.87388605]
 [0.12141801 0.878582  ]
 [0.13178092 0.86821914]
 [0.15786056 0.8421394 ]
 [0.15193059 0.84806937]
 [0.15434416 0.8456558 ]
 [0.21004558 0.7899544 ]
 [0.18097767 0.8190223 ]
 [0.16127205 0.83872795]
 [0.15520751 0.8447925 ]
 [0.1235871  0.8764129 ]
 [0.21296445 0.7870355 ]
 [0.16783932 0.83216065]
 [0.18169959 0.8183004 ]
 [0.37075236 0.6292476 ]
 [0.16860707 0.8313929 ]
 [0.13882475 0.86117524]
 [0.17433195 0.82566804]
 [0.13966373 0.86033624]
 [0.16830334 0.8316966 ]
 [0.10416561 0.8958343 ]
 [0.11158984 0.8884102 ]
 [0.13120066 0.8687994 ]
 [0.25234234 0.74765766]
 [0.13771607 0.8622839 ]
 [0.31367448 0.6863255 ]
 [0.16993181 0.83006823]
 [0.17314027 0.8268598 ]
 [0.12355871 0.8764413 ]
 [0.19910449 0.8008956 ]
 [0.10352676 0.89647317]
 [0.14278536 0.8572146 ]
 [0.24396735 0.75603265]
 [0.29656428 0.7034357 ]
 [0.08102439 0.91897565]
 [0.17148748 0.8285125 ]
 [0.25080237 0.7491976 ]
 [0.12813757 0.8718625 ]
 [0.10309482 0.8969052 ]
 [0.3832445  0.6167555 ]
 [0.19725208 0.8027479 ]
 [0.11250983 0.8874901 ]
 [0.18813448 0.81186557]
 [0.18017201 0.819828  ]
 [0.13305393 0.86694604]
 [0.25294062 0.7470593 ]
 [0.1566839  0.84331614]
 [0.18386008 0.81613994]
 [0.34310988 0.65689015]
 [0.2244056  0.7755944 ]
 [0.16653562 0.8334644 ]
 [0.25224376 0.7477562 ]
 [0.21441427 0.78558576]
 [0.21681386 0.78318614]
 [0.12639284 0.8736071 ]
 [0.25951073 0.7404893 ]
 [0.15095101 0.849049  ]
 [0.24129517 0.75870484]
 [0.09417864 0.9058214 ]
 [0.17130171 0.8286983 ]
 [0.1295994  0.87040067]
 [0.23776916 0.7622308 ]
 [0.22937727 0.7706227 ]
 [0.17661284 0.82338715]
 [0.11139876 0.88860124]
 [0.15680622 0.8431938 ]
 [0.13078402 0.869216  ]
 [0.13279355 0.8672065 ]
 [0.20372698 0.796273  ]
 [0.14972606 0.85027397]
 [0.3169705  0.6830295 ]
 [0.22707054 0.77292943]
 [0.19697203 0.8030279 ]
 [0.12175588 0.87824416]
 [0.19963546 0.80036455]
 [0.17032357 0.82967645]
 [0.09208069 0.90791935]
 [0.14676046 0.8532396 ]
 [0.21290167 0.78709835]
 [0.16650546 0.8334946 ]
 [0.20356819 0.79643184]
 [0.1117634  0.88823664]
 [0.1772737  0.8227263 ]
 [0.1740116  0.8259884 ]
 [0.15780804 0.842192  ]
 [0.13512282 0.86487716]
 [0.12277057 0.8772294 ]
 [0.22520725 0.77479273]
 [0.09078225 0.9092177 ]
 [0.16664554 0.8333545 ]
 [0.15303811 0.8469619 ]
 [0.1556607  0.8443393 ]
 [0.23244236 0.7675576 ]
 [0.15803006 0.84196997]
 [0.23199695 0.768003  ]
 [0.22054727 0.7794528 ]
 [0.20564535 0.7943546 ]
 [0.16195938 0.8380406 ]
 [0.19353512 0.80646497]
 [0.19879717 0.80120283]
 [0.2999068  0.7000932 ]
 [0.26789647 0.7321035 ]
 [0.20274822 0.79725176]
 [0.24683839 0.75316155]
 [0.17223565 0.8277643 ]
 [0.27186793 0.728132  ]
 [0.1747084  0.8252916 ]
 [0.21508196 0.784918  ]
 [0.20166543 0.79833454]
 [0.24699996 0.753     ]
 [0.09151082 0.9084892 ]
 [0.21627669 0.78372335]
 [0.34925547 0.65074456]
 [0.17550786 0.82449216]
 [0.17540178 0.8245982 ]
 [0.17690277 0.82309717]
 [0.21859168 0.78140837]
 [0.23654707 0.76345295]
 [0.1965715  0.8034285 ]
 [0.26539555 0.7346044 ]
 [0.22535302 0.774647  ]
 [0.20127742 0.79872257]
 [0.2558215  0.74417853]
 [0.2465872  0.7534128 ]
 [0.1635192  0.83648086]
 [0.13116428 0.86883575]
 [0.23934719 0.76065284]
 [0.22174324 0.7782568 ]
 [0.12891394 0.871086  ]
 [0.22286506 0.7771349 ]
 [0.14262214 0.8573778 ]
 [0.18301108 0.8169889 ]
 [0.28138378 0.7186162 ]
 [0.17566937 0.8243306 ]
 [0.16094501 0.83905494]
 [0.17268072 0.82731926]
 [0.254268   0.745732  ]
 [0.12807952 0.87192047]
 [0.17932974 0.8206703 ]
 [0.158481   0.841519  ]
 [0.17457037 0.8254297 ]
 [0.24477354 0.75522643]
 [0.12674612 0.8732539 ]
 [0.30411026 0.6958898 ]
 [0.1694677  0.8305323 ]
 [0.19428642 0.8057136 ]
 [0.17243715 0.8275628 ]
 [0.12788607 0.8721139 ]
 [0.18118201 0.81881803]
 [0.162248   0.8377519 ]
 [0.12320768 0.8767923 ]
 [0.16660063 0.83339936]
 [0.1407244  0.85927564]
 [0.14021808 0.859782  ]
 [0.16059306 0.8394069 ]
 [0.23093796 0.76906204]
 [0.19149958 0.80850047]
 [0.14227127 0.8577288 ]
 [0.1902106  0.80978936]
 [0.17578028 0.8242197 ]
 [0.16368587 0.83631414]
 [0.15169165 0.8483083 ]
 [0.21852219 0.7814778 ]
 [0.11629012 0.88370985]
 [0.21350613 0.78649384]
 [0.24635388 0.75364614]
 [0.12336716 0.8766328 ]
 [0.2062014  0.7937986 ]
 [0.24407347 0.7559266 ]
 [0.13909023 0.86090976]
 [0.16230819 0.8376918 ]
 [0.0720505  0.92794955]
 [0.11013512 0.8898649 ]
 [0.16943151 0.8305685 ]
 [0.08813182 0.91186816]
 [0.12320503 0.876795  ]
 [0.18761876 0.8123812 ]
 [0.16977751 0.8302225 ]
 [0.10384823 0.8961518 ]
 [0.14450958 0.85549045]
 [0.183827   0.81617296]
 [0.18652274 0.8134772 ]
 [0.24593754 0.7540624 ]
 [0.15430674 0.84569323]
 [0.29109237 0.70890766]]
Scores calculated from sklearn::
